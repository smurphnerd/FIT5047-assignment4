\section{Inhibiting Superpositions with Activations}
\label{sec:act_fns}
The paper presents the activation function as a key in enabling the formation of superpositions due to their ability to
filter out interference. As such, we demonstrate that modifying the activation can effectively control the model's usage of
superpositions.

We propose ExReLU, a modified version of ReLU with a cut-off threshold of $t \in \mathbb{R}$, instead of $0$. By setting $t < 0$, its filtering effect for negative interference is weakened, thus
increasing the cost for the model to adopt superpositional representations.
\[
	\text{ExReLU}(x) =
	\begin{cases}
		x & \text{if } x \geq t \\
		0 & \text{if } x < t
	\end{cases}
\]

\subsection{Model}
We trained two sets of models, each with hidden layers of two neurons on a dataset with five features whilst varying the feature
sparsity. The first set used the normal ReLU activation, while the second used ExReLU with $t=-0.25$.

\subsection{Results}
We visualized the hidden layer's representation of the features using the previously mentioned toy model framework.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{figures/acts_diagram.png}
	\caption{Comparison of feature embeddings by the two model sets, the code for which can be found in \href{https://colab.research.google.com/github/smurphnerd/FIT5047-assignment4/blob/main/act_visuals.ipynb}{this notebook}.}
	\label{fig:acts_diagram}
\end{figure}
Figure \ref{fig:acts_diagram} shows that ExReLU has successfully suppressed the model's usage of superpositions in this instance.
As such, it seems that the development of new activation functions akin to ExReLU may be key to ``solving superposition'' and controlling how models
use their available dimensions as mentioned in the paper.
