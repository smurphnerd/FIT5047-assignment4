\section{Review}
\label{sec:review}

The \textit{Toy Models of Superposition} paper \cite{elhage2022toymodelssuperposition} published in 2022 by researchers from Anthropic and Harvard University, investigates ``superposition'', a phenomenon where neural networks represent more features than they have available dimensions.
They define a \textit{feature} as an interpretable property of the input that neurons respond to.
The paper focuses on how neural networks overcome their limited dimensions through the use of polysemantic neurons, which represent multiple features at once.

%-------------------------------------------------------------------------

\subsection{Summary}

The authors use a toy model to demonstrate superposition by training it to project five features onto only two dimensions, then attempting to recover the original features.
One key finding is that the sparsity of a feature—how rarely it occurs—is a primary factor in determining whether it is represented in superposition. For instance, when features are not sparse, only the two most important ones are represented in dedicated orthogonal dimensions.
However, as sparsity increases, the model begins to encode all five features in superposition, arranging them geometrically in a pentagon-like structure within the two-dimensional space.
This results in interference between the features, where one's presence also activates other unrelated neurons.

The middle sections of the paper provide a mathematical explanation for why superposition occurs, whilst also exploring the specific ``phases'' of superposition that can occur under different conditions.
The paper concludes by discussing how ``solving superposition'' could lead to more interpretable and safer AI models.
They suggest that a key step in achieving this would be the ability to identify and enumerate all features in a model—what they refer to as the fundamental units of a neural network.
With such an understanding, one could potentially produce a universal quantifier over these features
This framework could then be used to fully explain model behaviour.
As a result, this would enable confidence that models would not simply avoid, but perhaps be entirely incapable of unethical behaviours like deception.
The authors propose that this might be achieved either by designing models that avoid superposition entirely or by finding an overcomplete basis that disentangles the superimposed features, motivating future research directions.

%-------------------------------------------------------------------------

\subsection{Significance}

The concept of superposition, as explored in this paper, holds significant implications for both our theoretical understanding of neural networks and practical advancements in mechanistic interpretability and AI safety.
It provides an entirely new perspective on the fundamental units of neural networks by showing how polysemantic neurons can be broken down into multiple features.
With over 200 citations in just 2 years, this paper has fueled a lot of modern day mechanistic interpretability research.

\subsection{Relevance in the Deep Learning Era}

In the current deep learning era, where advancements in model capabilities are positively correlated with model complexity, continuing to iterate this way only creates bigger, and bigger black boxes.
The insights this paper presents regarding the superposition phenomenon suggest alternate directions that would allow us to better understand these black boxes by ``solving superposition''.

Additionally, this paper shows the importance of model architecture, illuminating the tradeoff between efficiency brought about by smaller model sizes, and the interference caused by the superpositional feature representations that it necessitates.
As our understanding of model features matures, there is potential for model architecture hyperparams to be more analytically selected given information about the data, such as feature sparsity.
If we are able to obtain universal quantifiers over features as the paper suggests, we may even see a shift in the deep learning era where engineers can build or modify parts of these networks manually, having the ability to dictate the inclusion of specific features, and change the semanticity of neurons at will.
